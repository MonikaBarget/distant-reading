<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Understanding and plotting probability distributions in NLP | DistantReading</title>
    <meta name="description" content="Files and documentation for teaching">
    <meta name="generator" content="VitePress v1.4.1">
    <link rel="preload stylesheet" href="/distant-reading/assets/style.9s32jwnt.css" as="style">
    
    <script type="module" src="/distant-reading/assets/app.D-dSdnSN.js"></script>
    <link rel="preload" href="/distant-reading/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/distant-reading/assets/chunks/theme.BD7D8kYy.js">
    <link rel="modulepreload" href="/distant-reading/assets/chunks/framework.Ccoi24fT.js">
    <link rel="modulepreload" href="/distant-reading/assets/pages_nlp_plottingdistribution.md.Sbk9ob5E.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0f60ec36></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0f60ec36> Skip to content </a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-ab179fa1><a class="title" href="/distant-reading/" data-v-ab179fa1><!--[--><!--]--><!----><span data-v-ab179fa1>DistantReading</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/distant-reading/pages_datacollection.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Data Collection</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/distant-reading/pages_datacleaning.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Data Cleaning</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/distant-reading/pages_dataanalysis_samples.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Data Analysis Samples</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/distant-reading/pages_datasets_intro.html" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>Datasets Intro</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><!----><span data-v-cf11d7a2>Data Scraping</span><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_datascraping_applestore.html" data-v-35975db6><!--[--><span data-v-35975db6>Apple Store</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_datascraping_googlenews.html" data-v-35975db6><!--[--><span data-v-35975db6>Google News</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_datascraping_mastodon.html" data-v-35975db6><!--[--><span data-v-35975db6>Mastodon</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_datascraping_Meta.html" data-v-35975db6><!--[--><span data-v-35975db6>Meta</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_datascraping_reddit.html" data-v-35975db6><!--[--><span data-v-35975db6>Reddit</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_datascraping_twitter.html" data-v-35975db6><!--[--><span data-v-35975db6>Twitter</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_datascraping_youtube.html" data-v-35975db6><!--[--><span data-v-35975db6>YouTube</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup active" data-v-dc692963 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><!----><span data-v-cf11d7a2>NLP</span><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_nlp_segmentation.html" data-v-35975db6><!--[--><span data-v-35975db6>Segmentation</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_nlp_topicmodels.html" data-v-35975db6><!--[--><span data-v-35975db6>Topic Models</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link active" href="/distant-reading/pages_nlp_plottingdistribution.html" data-v-35975db6><!--[--><span data-v-35975db6>Plotting Distribution</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_nlp_multiplelanguages.html" data-v-35975db6><!--[--><span data-v-35975db6>Multiple Languages</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_nlp_spatialdata.html" data-v-35975db6><!--[--><span data-v-35975db6>Spatial Data</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><!----><span data-v-cf11d7a2>Skills</span><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_skills1_0_VPNclient.html" data-v-35975db6><!--[--><span data-v-35975db6>VPN Client</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_skills1_1_DSRI.html" data-v-35975db6><!--[--><span data-v-35975db6>DSRI</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_skills1_2a_scrapeAPPLEreviews.html" data-v-35975db6><!--[--><span data-v-35975db6>Scrape Apple Reviews</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_skills1_2b_scrapeYOUTUBEcomments.html" data-v-35975db6><!--[--><span data-v-35975db6>Scrape YouTube Comments</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_skills1_3_OpenRefine.html" data-v-35975db6><!--[--><span data-v-35975db6>OpenRefine</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_skills1_4_batchdownload.html" data-v-35975db6><!--[--><span data-v-35975db6>Batch Download</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_skills2_2_casestudy.html" data-v-35975db6><!--[--><span data-v-35975db6>Case Study</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_skills2_1_VoyantTools.html" data-v-35975db6><!--[--><span data-v-35975db6>Voyant Tools</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_skills3_presentations.html" data-v-35975db6><!--[--><span data-v-35975db6>Presentations</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-dc692963 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><!----><span data-v-cf11d7a2>Task Sheets</span><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_bodyimage.html" data-v-35975db6><!--[--><span data-v-35975db6>Body Image</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_domesticviolence.html" data-v-35975db6><!--[--><span data-v-35975db6>Domestic Violence</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_elonmusk.html" data-v-35975db6><!--[--><span data-v-35975db6>Elon Musk</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_girlboss.html" data-v-35975db6><!--[--><span data-v-35975db6>Girlboss</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_epstein.html" data-v-35975db6><!--[--><span data-v-35975db6>Jeffrey Epstein</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_menstruation.html" data-v-35975db6><!--[--><span data-v-35975db6>Menstruation</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_mileycyrus.html" data-v-35975db6><!--[--><span data-v-35975db6>Miley Cyrus</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_race.html" data-v-35975db6><!--[--><span data-v-35975db6>Race</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_seretsekhama.html" data-v-35975db6><!--[--><span data-v-35975db6>Seretse Khama</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_slavery.html" data-v-35975db6><!--[--><span data-v-35975db6>Slavery</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_sorayaesfandiary.html" data-v-35975db6><!--[--><span data-v-35975db6>Soraya Esfandiary</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_truecrime.html" data-v-35975db6><!--[--><span data-v-35975db6>True Crime</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_wastecolonialism.html" data-v-35975db6><!--[--><span data-v-35975db6>Waste Colonialism</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/distant-reading/pages_tasksheet_witchcraft.html" data-v-35975db6><!--[--><span data-v-35975db6>Witchcraft</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/vuejs/vitepress" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><span class="vpi-social-github" /></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/vuejs/vitepress" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-eee4e7cb><span class="vpi-social-github" /></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-17a5e62e><button data-v-17a5e62e>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0" data-v-c40bc020 data-v-b7550ba0><div class="item" role="button" tabindex="0" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><h2 class="text" data-v-b7550ba0>External Links</h2><!----></div><div class="items" data-v-b7550ba0><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link vp-external-link-icon link" href="https://github.com/MonikaBarget/distant-reading" target="_blank" rel="noreferrer" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>GitHub Code</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b7550ba0 data-v-b7550ba0><div class="item" data-v-b7550ba0><div class="indicator" data-v-b7550ba0></div><a class="VPLink link vp-external-link-icon link" href="https://www.youtube.com/@digitalhistory7990" target="_blank" rel="noreferrer" data-v-b7550ba0><!--[--><p class="text" data-v-b7550ba0>YouTube Channel</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _distant-reading_pages_nlp_plottingdistribution" data-v-39a288b8><div><h1 id="understanding-and-plotting-probability-distributions-in-nlp" tabindex="-1">Understanding and plotting probability distributions in NLP <a class="header-anchor" href="#understanding-and-plotting-probability-distributions-in-nlp" aria-label="Permalink to &quot;Understanding and plotting probability distributions in NLP&quot;">​</a></h1><h2 id="definition-of-probability-distribution" tabindex="-1">Definition of probability distribution <a class="header-anchor" href="#definition-of-probability-distribution" aria-label="Permalink to &quot;Definition of probability distribution&quot;">​</a></h2><p>According to the Geeks for Geeks article <a href="https://www.geeksforgeeks.org/statistics-in-natural-language-processing/" target="_blank" rel="noreferrer">Fundamentals of statistics in natural langauge processing</a>, a variety of statistical concepts and methods are used in NLP. One aspect are so-called <em>descriptive statistics</em>, which include frequency counts (e.g. represented in word clouds), measures of Central Tendency (mean, median, mode), and measures of dispersion (e.g. for analysing &quot;variability in word or sentence lengths&quot;). Another important statistical concept often used in NLP are so-called <em>probability distributions</em>. Probability distributions are used in NLP to model the natural variation and predictability of language.</p><p>Julia Hockenmaier, lecturer at the Sibel Center, University of Illinois, has outlined the most common probability models for NLP in <a href="https://courses.grainger.illinois.edu/cs447/fa2020/Slides/Lecture03.pdf" target="_blank" rel="noreferrer">a lecture for the course CS447: Natural Language Processing</a>. This lecture introduces students of computer science and computational linguistics to how language models &quot;define probability distributions over the strings in a language&quot; (slide 5), explains n-gram models and outlines &quot;some very basic probability theory&quot;. Slides 11 and 12 explain sampling with replacement (often covered in secondary school mathematics courses as well) and applies this process to a text as a &quot;bag of words&quot; (slide 13).</p><p>Slide 16 defines a probability distribution as frequently used in NLP. In natural language processing, we often work with probability distributions to describe how likely different outcomes are. A probability distribution assigns a value between 0 (0%) and 1 (100%) to each possible outcome. In language modelling, these outcomes are typically words or sequences of words. For example, NLP models, trained on typical language use, assign a high probability to “the” following “of,” and a lower probability to the word “banana.” This is also why you typically see decimal values below 1 when looking at statistical information in Voyant Tools. The space over which this distribution is defined is often called omega (Ω). This typically refers to all possible words or sentences within a language.</p><h3 id="discrete-probability" tabindex="-1">Discrete probability <a class="header-anchor" href="#discrete-probability" aria-label="Permalink to &quot;Discrete probability&quot;">​</a></h3><p>Hockenmaier (slide 17) further introduces the concept of discrete probability, which refers to situations where there is a fixed number of possible outcomes. This is relevant to NLP because words are discrete units. A sentence consists of individual word tokens drawn from a known vocabulary. Inventing new words is possible but uncommon. The Bernoulli distribution is the simplest discrete probability distribution, defined over two outcomes, such as success/failure, yes/no, or true/false. While not unique to NLP, the Bernoulli distribution is frequently used when modelling binary decisions, such as whether a sentence ends or not. The categorical distribution is a generalisation of the Bernoulli: it describes the probability of one outcome from multiple categories. In NLP, this is used to model the probability of selecting a word from a vocabulary. These distributions allow language models to quantify uncertainty and variation in a structured way, based on observed data.</p><h3 id="joint-probability" tabindex="-1">Joint probability <a class="header-anchor" href="#joint-probability" aria-label="Permalink to &quot;Joint probability&quot;">​</a></h3><p>Joint probability (Hockenmaier, slides 18–19) refers to the likelihood of two or more events occurring together. In the context of language, this could mean the probability of a word appearing in a certain position and another word appearing in relation to it: for instance, the probability of the phrase “war and peace.” The chain rule is a fundamental concept from probability theory that allows us to calculate the joint probability of a sequence of events (or words) by expressing it as a product of conditional probabilities. In practice, this means we break down the probability of a sentence into the probability of each word given the previous ones.</p><p>However, calculating these probabilities can quickly become unmanageable due to the vast number of possible word combinations. This is why many language models make so-called independence assumptions. These assumptions simplify the model by pretending that certain parts of the language are independent — even if they aren’t in reality. This drastically reduces the number of parameters (probabilities) the model needs to estimate. It allows the model to be trained with less data, although at the cost of accuracy. These simplifications are common in statistical models, not just in NLP.</p><h3 id="constructing-efficient-nlp-models-and-limits-of-reliability" tabindex="-1">Constructing efficient NLP models and limits of reliability <a class="header-anchor" href="#constructing-efficient-nlp-models-and-limits-of-reliability" aria-label="Permalink to &quot;Constructing efficient NLP models and limits of reliability&quot;">​</a></h3><p>To build a useful probability model for language, two things are required: a model definition and parameter estimation. (Hockenmaier, slide 21) The model definition outlines the structure, for example, whether the model assumes each word depends on the last one or the last three. Parameter estimation refers to the process of calculating the actual probabilities from a dataset, usually based on how often certain word combinations appear in real texts. In practice, almost all language models make independence assumptions to make the model simpler and more trainable. Without them, the number of probabilities to estimate would be enormous, and we would never have enough training data to fill in all the necessary details. However, these assumptions are never perfect, and they lead to models that occasionally make unrealistic predictions — such as assigning high probabilities to ungrammatical or nonsensical sentences.</p><h3 id="n-gram-models-for-word-prediction" tabindex="-1">n-gram models for word prediction <a class="header-anchor" href="#n-gram-models-for-word-prediction" aria-label="Permalink to &quot;n-gram models for word prediction&quot;">​</a></h3><p>An n-gram model is a type of statistical language model used to predict the next word in a sequence. The “n” in n-gram refers to the number of words considered:</p><blockquote>An n-gram language model assumes each word epends only on the last n−1 words.</blockquote> (Hockenmaier, slide 23) <p>A bigram model (n=2) predicts each word based on the word that comes before it. A trigram model (n=3) considers the two previous words, and so on. This approach is a simplification of real language, which often depends on more than just the last few words. However, n-gram models are popular because they are relatively easy to train and interpret. They balance complexity with practicality: more context improves prediction, but too much context makes the model harder to manage.</p><h3 id="approaching-language-from-a-mathematical-perspective" tabindex="-1">Approaching language from a mathematical perspective <a class="header-anchor" href="#approaching-language-from-a-mathematical-perspective" aria-label="Permalink to &quot;Approaching language from a mathematical perspective&quot;">​</a></h3><p>It is important to remember that NLP depends on an abstract, mathematical approach to language. In these terms, a language model is a function that assigns probabilities to sequences of words. It tells us how likely it is that a particular sentence will occur. The model is trained on a vocabulary (meaning a fixed set of words) and can then be used to generate or analyze text. (Hockenmaier, slide 29)</p><p>The mathematical notation V* refers to all possible strings (sentences) that can be made from a vocabulary V. The model assigns probabilities to these strings, ideally in a way that reflects how people actually use language. One challenge is that we want a single model that works for strings of any length, not just for fixed-length sequences. This is where mathematical concepts like recursive probability estimation come in: we multiply the probabilities of each word given its history, and this allows us to generate or evaluate complete sentences.</p><h3 id="beginning-and-end-of-sentence-markers-bos-and-eos" tabindex="-1">Beginning and End of Sentence Markers (BOS and EOS) <a class="header-anchor" href="#beginning-and-end-of-sentence-markers-bos-and-eos" aria-label="Permalink to &quot;Beginning and End of Sentence Markers (BOS and EOS)&quot;">​</a></h3><p>To train a language model effectively, we need to tell it where sentences begin and end. This is done by adding special tokens — BOS (beginning-of-sentence) and EOS (end-of-sentence) — to the training data. These markers are not real words but serve a crucial role: they let the model learn that language has structure, and that word choices often depend on the start and end of a sentence. (Hockenmaier, slide 32) By including these tokens, the model learns not only what words typically follow others, but also how sentences are shaped — where they tend to start and how they tend to conclude.</p><h3 id="maximum-likelihood-estimation-mle" tabindex="-1">Maximum Likelihood Estimation (MLE) <a class="header-anchor" href="#maximum-likelihood-estimation-mle" aria-label="Permalink to &quot;Maximum Likelihood Estimation (MLE)&quot;">​</a></h3><p>A large amount of text is needed as training data to &quot;learn&quot; or &quot;estimate&quot; the parameters typical of a language model and develop reliable probabilities. One of the simplest and most common ways to estimate probabilities in a language model is called Maximum Likelihood Estimation, or MLE. This technique is not unique to NLP — it is a core statistical method used in many fields. MLE works by using observed data to estimate how often each event occurs. In NLP, this usually means counting how often a word or phrase appears in a large corpus, and dividing that count by the total number of observations. The result is a probability: the likelihood that a given word will appear in a certain context. While MLE is easy to understand and implement, it does have limitations, especially when dealing with rare or unseen word combinations. More advanced models use smoothing techniques to handle this, but those are beyond the scope of this course. (Hockenmaier, slide 34)</p><h3 id="generating-text-with-language-models" tabindex="-1">Generating Text with Language Models <a class="header-anchor" href="#generating-text-with-language-models" aria-label="Permalink to &quot;Generating Text with Language Models&quot;">​</a></h3><p>As you will know from your own use of AI chatbots etc., language models are not only used for analyzing existing texts — they can also be used to generate new sentences. By sampling from the model’s probability distributions, we can create strings of words that resemble natural language. Building grammatically correct strings is fairly easy, but selecting words that make sense to humans in combination requires advanced models and extensive training.</p><p>The examples given on Hockenmaier&#39;s slides do not necessarily make sense but show typical features of the text types chosen, e.g. frequently used vocabulary in a business journal or a Shakespeare work. Differences between adjectives, verbs, adverbs and nouns are not recognized to a sufficient degree, but the quadrigram output is more logical than the unigram output that just puts individual words behind each other. A quadigram output includes grammatically correct sentences like &quot;Will you not tell me who I am?&quot;.</p><p>The quality of the generated text, therefore, depends on the kind of model used, and the public accessibility of LLMs through tools like ChatGPT has made this very transparent in recent years. A unigram model (where each word is chosen independently) typically produces random word sequences with little structure. A quadgram model, on the other hand, uses more context and often produces much more coherent and grammatically correct output. This illustrates how important context is in language generation: the more of the sentence the model can &quot;see,&quot; the more natural the output tends to be. (Hockenmaier, slide 39)</p><h3 id="evaluating-language-models" tabindex="-1">Evaluating Language Models <a class="header-anchor" href="#evaluating-language-models" aria-label="Permalink to &quot;Evaluating Language Models&quot;">​</a></h3><p>Finally, language models must be evaluated to understand how well they perform. There are two main approaches:</p><ul><li><p>Intrinsic evaluation looks at how well the model performs a specific task, such as predicting the next word in a sentence.</p></li><li><p>Extrinsic evaluation measures how useful the model is in a larger system — for example, whether it improves the quality of a translation system or a search engine.</p></li></ul><p>Evaluation is one of the most difficult parts of working with language models because human language is so flexible and nuanced. Different evaluation methods highlight different strengths and weaknesses in a model, and choosing the right method depends on the goals of your project. (Hockenmaier, slide 50)</p><h2 id="making-working-with-complex-textual-data-easier-by-reduction" tabindex="-1">Making working with complex textual data easier by reduction <a class="header-anchor" href="#making-working-with-complex-textual-data-easier-by-reduction" aria-label="Permalink to &quot;Making working with complex textual data easier by reduction&quot;">​</a></h2><p>To make texts consisting in many different words easier to visualise, <em>dimensionality reduction</em> is applied. This technique reduces high-dimensional data (data with a lot of information) to representative core features so that we can understand it more easily. In NLP, you often find dimensionality reduction using the Principal Component Analysis (PCA) method (see section on scatterplots), which finds the most important features or directions in the data. It keeps as much of the original information as possible. In NLP, PCA takes high-dimensional word representations (embeddings) and shows them in 2D or 3D plots so we can see patterns and relationships more clearly. t-SNE (t-Distributed Stochastic Neighbor Embedding) is another technique of dimension reduction, but it functions differently and creates semantic distributions. It focuses on keeping similar items close together. It is used for showing clusters of words or groups of documents in 2D or 3D.</p><h2 id="distributed-representations-embeddings" tabindex="-1">Distributed representations / embeddings <a class="header-anchor" href="#distributed-representations-embeddings" aria-label="Permalink to &quot;Distributed representations / embeddings&quot;">​</a></h2><p>In <em>distributed representations</em>, also known as <em>embeddings</em>, the idea is that the &quot;meaning&quot; or &quot;semantic content&quot; of a data point is distributed across multiple dimensions. Expressing it more simply, distributed representations or embeddings are a way of representing words as numbers. These numbers, or vectors, are like coordinates in a space where each word is a point. Words that have similar meanings are placed near each other in this space. For example, the words &quot;cat&quot; and &quot;dog&quot; might be close together because they are both animals, while &quot;cat&quot; and &quot;car&quot; might be farther apart, since they&#39;re not related in meaning. There are special techniques, like Word2Vec, GloVe, and FastText, that learn how to place words in this space by looking at lots of text. The space can be a two-dimensional graph with an x- and a y-axes, with the different dots arranged between these axes. A plot like this is called a scatter plot.</p><h2 id="reading-scatter-plots" tabindex="-1">Reading scatter plots <a class="header-anchor" href="#reading-scatter-plots" aria-label="Permalink to &quot;Reading scatter plots&quot;">​</a></h2><p>In a scatter plot, the x-axis (horizontal) and y-axis (vertical) each represent a variable. Each dot drawn in a scatter plot corresponds to a unique observation in both dimensions. These dimensions can be absolute numbers, e.g. a person&#39;s age and income, or relative values, e.g. when we are considering word distrubitions in a text or a collection of texts. When points group together in a scatter plot, it suggests a concentration of data in that range.</p><p>📈 If the points generally point upward (from bottom left to top right), it shows a positive correlation, meaning as one variable increases, the other increases, too. One example can be the relationship between study time and grades. If we find that students who study longer also have better grades in the exams, we speak of a positive correlation.</p><p>📉 A downward slope shows a negative correlation. One example of a negative correlation can be screen time versus sleep hours. As screen time increases, sleep tends to decrease as well.</p><p>If the points are scattered randomly with no clear slope or pattern, this indicates no relationship between the variables.</p><p>📈 add sample graph for number of pets and student grades</p><p>Some points may sit far away from the cluster, which could be outliers — unusual observations that differ significantly from the trend. These might signal unique cases or errors in data.</p><p>📋 find non-NLP example</p><p>Understanding scatter plots is not just about observing the dominant trend but about interpreting what the relationships of all points (or lack thereof) means. Ask questions like: “What could a strong positive correlation tell us about these two variables?”</p><p>In NLP, scatter plots are commonly used to show relationships or similarities between words, sentences, or entire documents. Each point represents a word, sentence, or document based on its embedding or similarity score. In contrast to a scatter plot in which values such as age, income, height, weight, or length are measured, an NLP scatter plot can have negative values on both the x and y axes because we deal with probability measures. Negative values represent dissimilarity where positive values express similarity.</p><p>📋 find non-NLP example based on data used for teaching</p><p>Depending on the context and the algorithms used, NLP scatter plots do not always represent the exact same values but can entail very different properties, like two different sentiment scores (e.g., positive and negative) or dimensions of similarity. So be carefuly to check what exactly is being shown before you begin your analysis!</p><p>In NLP scatter plots, clustering often indicates semantic similarity rather than direct co-occurrence in the text or corpus itself. The distance between points can then signal semantic similarity (closer points mean more similar meaning or sentiment). For example, &quot;doctor&quot; and &quot;nurse&quot; or &quot;master&quot; and &quot;slave&quot; might cluster together due to shared contexts in training data, but they don’t necessarily appear together in the text.</p><p>📋 add example using Voyant</p><p>This is why NLP scatter plots are similar to mind maps or conceptual maps rather than a structured representation of the text. Some algorithms may also result in point / words clusters in which words behave similarly in statistical terms while there is no content-based relationship between them.</p><p>📋 add example chart</p><p>Moreover, scatter plots in NLP sometimes reveal linear trends if there’s a direct relationship between two similarity measures, but more often display nonlinear clusters if embeddings capture complex, multidimensional meanings. Points far from clusters may represent unusual or ambiguous words, phrases, or sentences that do not fit into clear similarity groups. For example, highly context-dependent words or polysemous words (words with multiple meanings) might appear as outliers, indicating that they don’t neatly belong to a single cluster.</p><p>Special techniques, like Word2Vec, GloVe, and FastText, place words in the plot placed on word meanings and identify similar words, classify entire texts, or even translating languages. Some types of embeddings, like contextual embeddings (from models like BERT or GPT), do take the specific context into account, meaning each word has a unique embedding in each context. However, even in these cases, clustering will reflect shared meaning across contexts rather than specific occurrences within a particular text passage.</p><h2 id="understanding-the-scatter-plot-tool-in-voyant" tabindex="-1">Understanding the scatter plot tool in Voyant <a class="header-anchor" href="#understanding-the-scatter-plot-tool-in-voyant" aria-label="Permalink to &quot;Understanding the scatter plot tool in Voyant&quot;">​</a></h2><p>Unfortunately, the Voyant documentation for the scatter plot tool is quite abstract and short. It does not providing a lot of insight for beginners and uses many technical terms without definitions. According to this documentation, the Voyant scatter plots allow for several distribution techniques.</p><p>The first one is <strong>Principal Component Analysis (PCA)</strong>, which &quot;takes data in a multidimensional space and optimizes it, reducing the dimensions to a manageable subset. It is a way of transforming the data with respect to its own structure, so that associations between data points become more readily apparent.&quot; (Voyant Documentation)</p><p>Breaking this down, it is important to understand that a multidimensional space in data analysis is a way to describe data that has many different features. For example, each document in a collection might be considered one “dimension.” Too many dimensions make it hard to visualize patterns as you will see so many points in a chart that all the data overlap or that the scale becomes too small to read. This is why data need to be reduced and represented by fewer data points. Principal Component Analysis (PCA) is one such method that simplifies complex data by focussing on the biggest patterns in how things differ (variability / variance), so that they can be shown on a human-readable 2D or 3D plot.</p><p><strong>Correspondence Analysis (CA)</strong> is another distribution method that you can select in the Voyant scatter plats. It attempts data reduction similar to PCA, but it shows both words and documents in the same plot. You can thus see which words are typical of which documents. <strong>Document Similarity (DS)</strong> is a variant of Correspondence Analysis that only shows documents (no word labels). It shows which documents use language in statistically similar ways.</p><h3 id="options-in-the-voyant-tools-scatter-plots" tabindex="-1">Options in the Voyant Tools scatter plots <a class="header-anchor" href="#options-in-the-voyant-tools-scatter-plots" aria-label="Permalink to &quot;Options in the Voyant Tools scatter plots&quot;">​</a></h3><p>The toolbar mainly comprises options for tweaking and exploring the plotting of the graph.</p><ul><li><em>Analysis</em> allows the user to switch between plotting Document Similarity, Principal Component Analysis and Correspondence Analysis (see explanations in the section above)</li><li><em>Clusters</em> allows the user to control the number of groups to cluster the words into. These clusters are determined automatically by the criteria of the analysis and words in a cluster would indicate a measure of similarity between words. Clusters of terms will appear as a single colour.</li><li><em>Dimensions</em> allows the user to switch between two or three dimensions.</li><li><em>Labels</em> allows the user to cycle through the label settings for the graph.</li></ul><h3 id="terms" tabindex="-1">Terms <a class="header-anchor" href="#terms" aria-label="Permalink to &quot;Terms&quot;">​</a></h3><p>The Terms panel shows you which terms are displayed in the current scatterplot and it also allows you to control which terms are shown. The terms grid functions like other grids in Voyant and you can sort terms alphabetically or by frequency. The Terms panel also provides the following functionality:</p><ul><li><em>Terms Count</em>: determine how many terms to display at once in the graph (the terms present will influence the layout of the terms, so it&#39;s well worth experimenting with this option)</li><li><em>Nearby</em>: you can select a term of interest from the grid and ask to zoom in on &quot;nearby&quot; terms (terms that cluster in proximity)</li><li><em>Remove</em>: you can remove one term at a time by selecting it in the grid and hitting the Remove button</li><li><em>Add Term</em>: you can search for and add new terms</li></ul><p>🛠️ Under construction</p><h3 id="works-cited-and-recommendations-for-further-reading" tabindex="-1">Works cited and recommendations for further reading <a class="header-anchor" href="#works-cited-and-recommendations-for-further-reading" aria-label="Permalink to &quot;Works cited and recommendations for further reading&quot;">​</a></h3><p>Fundamentals of statistics in natural language processing(Nlp). (2024, July 15). GeeksforGeeks. <a href="https://www.geeksforgeeks.org/statistics-in-natural-language-processing" target="_blank" rel="noreferrer">https://www.geeksforgeeks.org/statistics-in-natural-language-processing</a></p><p>Recommended lecture on <a href="https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;opi=89978449&amp;url=https://courses.grainger.illinois.edu/cs447/fa2020/Slides/Lecture03.pdf&amp;ved=2ahUKEwibh46n28yJAxXx9bsIHauuDNkQFnoECBMQAQ&amp;usg=AOvVaw2q5YGp2ei5kG-8cN3PGHY5" target="_blank" rel="noreferrer">Probability Models for NLP</a></p></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><!----></div><div class="pager" data-v-e257564d><a class="VPLink link vp-external-link-icon pager-link next" href="https://github.com/MonikaBarget/distant-reading" target="_blank" rel="noreferrer" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>GitHub Code</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"lh-yEOY3\",\"index.md\":\"CWoAwYm5\",\"markdown-examples.md\":\"CATt-Usg\",\"pages_dataanalysis_samples.md\":\"Cj0ejkYY\",\"pages_datacleaning.md\":\"BB4XLPGt\",\"pages_datacollection.md\":\"CFbQKZLJ\",\"pages_datascraping_applestore.md\":\"C_7mwHHH\",\"pages_datascraping_googleapi.md\":\"aLJzD2kJ\",\"pages_datascraping_googlenews.md\":\"BqYKERQW\",\"pages_datascraping_mastodon.md\":\"BJsrUEtu\",\"pages_datascraping_meta.md\":\"Ccle9Oej\",\"pages_datascraping_reddit.md\":\"Bx5cwmwz\",\"pages_datascraping_twitter.md\":\"CL3nIHhx\",\"pages_datascraping_youtube.md\":\"DkJkDyl7\",\"pages_datasets_intro.md\":\"Xa5-___l\",\"pages_nlp_introduction.md\":\"CYhaUC_W\",\"pages_nlp_multiplelanguages.md\":\"DTnoY5QC\",\"pages_nlp_plottingdistribution.md\":\"Sbk9ob5E\",\"pages_nlp_segmentation.md\":\"CDY22Kyx\",\"pages_nlp_spatialdata.md\":\"CAAu6oBi\",\"pages_nlp_topicmodels.md\":\"NLlsTG-u\",\"pages_skills1_0_vpnclient.md\":\"CUEeE0Fs\",\"pages_skills1_1_dsri.md\":\"DW4ltwBt\",\"pages_skills1_2a_scrapeapplereviews.md\":\"BMrcFNWX\",\"pages_skills1_2b_scrapeyoutubecomments.md\":\"FvinYHHR\",\"pages_skills1_3_openrefine.md\":\"BdnPRtCl\",\"pages_skills1_4_batchdownload.md\":\"ClwwFpsa\",\"pages_skills2_1_voyanttools.md\":\"wLEHHK6k\",\"pages_skills2_2_casestudy.md\":\"Cbc9_XYi\",\"pages_skills3_presentations.md\":\"DiEJadje\",\"pages_tasksheet_bodyimage.md\":\"DMsBN7rR\",\"pages_tasksheet_domesticviolence.md\":\"6EfgRd3w\",\"pages_tasksheet_elonmusk.md\":\"Ojv-FkUQ\",\"pages_tasksheet_epstein.md\":\"BVhVt9bS\",\"pages_tasksheet_girlboss.md\":\"B6PeUZP3\",\"pages_tasksheet_menstruation.md\":\"Dpix_Ksb\",\"pages_tasksheet_mileycyrus.md\":\"DSSDvTkm\",\"pages_tasksheet_race.md\":\"D8n4y0rB\",\"pages_tasksheet_seretsekhama.md\":\"Bujd4t55\",\"pages_tasksheet_slavery.md\":\"CGZMA747\",\"pages_tasksheet_sorayaesfandiary.md\":\"B_2hupuq\",\"pages_tasksheet_truecrime.md\":\"CURIh1_4\",\"pages_tasksheet_wastecolonialism.md\":\"DWRNrmjR\",\"pages_tasksheet_witchcraft.md\":\"BhxoUk_L\",\"pages_welcome.md\":\"B6GL1-S-\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"DistantReading\",\"description\":\"Files and documentation for teaching\",\"base\":\"/distant-reading/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"Data Collection\",\"link\":\"/pages_datacollection.md\"},{\"text\":\"Data Cleaning\",\"link\":\"/pages_datacleaning.md\"},{\"text\":\"Data Analysis Samples\",\"link\":\"/pages_dataanalysis_samples.md\"},{\"text\":\"Datasets Intro\",\"link\":\"/pages_datasets_intro.md\"},{\"text\":\"Data Scraping\",\"items\":[{\"text\":\"Apple Store\",\"link\":\"/pages_datascraping_applestore.md\"},{\"text\":\"Google News\",\"link\":\"/pages_datascraping_googlenews.md\"},{\"text\":\"Mastodon\",\"link\":\"/pages_datascraping_mastodon.md\"},{\"text\":\"Meta\",\"link\":\"/pages_datascraping_Meta.md\"},{\"text\":\"Reddit\",\"link\":\"/pages_datascraping_reddit.md\"},{\"text\":\"Twitter\",\"link\":\"/pages_datascraping_twitter.md\"},{\"text\":\"YouTube\",\"link\":\"/pages_datascraping_youtube.md\"}]},{\"text\":\"NLP\",\"items\":[{\"text\":\"Segmentation\",\"link\":\"/pages_nlp_segmentation.md\"},{\"text\":\"Topic Models\",\"link\":\"/pages_nlp_topicmodels.md\"},{\"text\":\"Plotting Distribution\",\"link\":\"/pages_nlp_plottingdistribution.md\"},{\"text\":\"Multiple Languages\",\"link\":\"/pages_nlp_multiplelanguages.md\"},{\"text\":\"Spatial Data\",\"link\":\"/pages_nlp_spatialdata.md\"}]},{\"text\":\"Skills\",\"items\":[{\"text\":\"VPN Client\",\"link\":\"/pages_skills1_0_VPNclient.md\"},{\"text\":\"DSRI\",\"link\":\"/pages_skills1_1_DSRI.md\"},{\"text\":\"Scrape Apple Reviews\",\"link\":\"/pages_skills1_2a_scrapeAPPLEreviews.md\"},{\"text\":\"Scrape YouTube Comments\",\"link\":\"/pages_skills1_2b_scrapeYOUTUBEcomments.md\"},{\"text\":\"OpenRefine\",\"link\":\"/pages_skills1_3_OpenRefine.md\"},{\"text\":\"Batch Download\",\"link\":\"/pages_skills1_4_batchdownload.md\"},{\"text\":\"Case Study\",\"link\":\"/pages_skills2_2_casestudy.md\"},{\"text\":\"Voyant Tools\",\"link\":\"/pages_skills2_1_VoyantTools.md\"},{\"text\":\"Presentations\",\"link\":\"/pages_skills3_presentations.md\"}]},{\"text\":\"Task Sheets\",\"items\":[{\"text\":\"Body Image\",\"link\":\"/pages_tasksheet_bodyimage.md\"},{\"text\":\"Domestic Violence\",\"link\":\"/pages_tasksheet_domesticviolence.md\"},{\"text\":\"Elon Musk\",\"link\":\"/pages_tasksheet_elonmusk.md\"},{\"text\":\"Girlboss\",\"link\":\"/pages_tasksheet_girlboss.md\"},{\"text\":\"Jeffrey Epstein\",\"link\":\"/pages_tasksheet_epstein.md\"},{\"text\":\"Menstruation\",\"link\":\"/pages_tasksheet_menstruation.md\"},{\"text\":\"Miley Cyrus\",\"link\":\"/pages_tasksheet_mileycyrus.md\"},{\"text\":\"Race\",\"link\":\"/pages_tasksheet_race.md\"},{\"text\":\"Seretse Khama\",\"link\":\"/pages_tasksheet_seretsekhama.md\"},{\"text\":\"Slavery\",\"link\":\"/pages_tasksheet_slavery.md\"},{\"text\":\"Soraya Esfandiary\",\"link\":\"/pages_tasksheet_sorayaesfandiary.md\"},{\"text\":\"True Crime\",\"link\":\"/pages_tasksheet_truecrime.md\"},{\"text\":\"Waste Colonialism\",\"link\":\"/pages_tasksheet_wastecolonialism.md\"},{\"text\":\"Witchcraft\",\"link\":\"/pages_tasksheet_witchcraft.md\"}]}],\"sidebar\":[{\"text\":\"External Links\",\"items\":[{\"text\":\"GitHub Code\",\"link\":\"https://github.com/MonikaBarget/distant-reading\"},{\"text\":\"YouTube Channel\",\"link\":\"https://www.youtube.com/@digitalhistory7990\"}]}],\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/vuejs/vitepress\"}]},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>